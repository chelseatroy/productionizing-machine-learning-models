## Prerequisite steps to run this repo:

1. [Create an AWS account](console.aws.amazon.com)
2. [Make an S3 bucket](https://s3.console.aws.amazon.com/s3/)
3. Put the bucket name in the command at the bottom of the Dockerfile
4. Put the bucket's REGION in start_docker.sh
5. Create an IAM User:

### ğŸ” Step-by-Step: Create a New IAM User with S3 Access

1. **Sign into the AWS Management Console**  
   Go to: [https://console.aws.amazon.com/iam/](https://console.aws.amazon.com/iam/)

2. **Go to Users**  
   In the left sidebar, click **Users**, then click **â€œAdd usersâ€**.

3. **Set Username & Access Type**
   - **User name**: something like `metaflow-user`
   - **Access type**: âœ… **Programmatic access**  
     (this gives you an access key & secret key for use in code)

4. **Set Permissions**
   - Choose **â€œAttach policies directlyâ€**
   - To keep it simple, search for and check âœ… **AmazonS3FullAccess**  
     (You can restrict this later to specific buckets if needed.)

5. **Skip tags** (optional)

6. **Review & Create**
   - Click **Create user**

7. **Save the credentials!**
   - On the success screen, youâ€™ll see:
     - **Access key ID**
     - **Secret access key**
   - ğŸ“¥ **Download the CSV** or copy them somewhere safe â€” you wonâ€™t see the secret again later.

8. **Put the access key ID and secret access key into the appropriate env vars in start_docker.sh.**

## Okay, time to run the repo!

## ğŸš€ How to Build, Train, and Launch the Dashboard

### ğŸ›  Step 1: Run the setup script

From the root of the project:

```bash
./setup_and_build.sh     # for macOS/Linux
```

or for windows:

```bash
./setup_and_build.ps1    # for Windows
```

This will:
   - Download and cache the IMDB dataset locally
   - Write Dockerfiles for the flow and dashboard
   - Build the Docker images for both components

### ğŸ§± Step 2: Start all services

From the root of the project:

```bash
docker compose up -d
```

This runs:
   - metaflow-pipeline: a container for running the ML pipeline
   - streamlit-dashboard: the web app at http://localhost:8501

### ğŸ§ª Step 3: Train the model

Open a shell inside the Metaflow container:

```
docker exec -it metaflow-pipeline bash
```

Then run the Metaflow flow:

```
python sentiment_analysis_flow.py run --s3-bucket $S3_BUCKET
```

âœ… Make sure your .env file contains a valid S3 bucket name:

```
S3_BUCKET=your-s3-bucket-name
```

This command will:
   - Fetch and preprocess book reviews
   - Train a logistic regression model
   - Evaluate the model on validation data
   - Upload metrics, model, vectorizer, and confusion matrix to S3

### ğŸ“Š Step 4: View the dashboard

Open your browser and visit:

```
http://localhost:8501
```

Youâ€™ll see:
   - ğŸ“Š Evaluation metrics (accuracy, f1_score)
   - ğŸ“‰ Confusion matrix image
   - âœï¸ Text box for testing new reviews
   - ğŸ”® Sentiment prediction for input

### ğŸ§¹ Optional: Stop all containers

```
docker compose down
```


## Commands you should not need, but I'm documenting just in case:

To download the imdb data (which is done for you already in the pushed version of this repository):
`python -c "from datasets import load_dataset; load_dataset('imdb', cache_dir='./hf_cache')"`
